{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from epoxy import *\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: FlyingSquid\n",
    "\n",
    "First, we load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train_orig = np.load('data/L_train.npy')\n",
    "L_valid_orig = np.load('data/L_valid.npy')\n",
    "L_test_orig = np.load('data/L_test.npy')\n",
    "\n",
    "Y_valid = np.load('data/Y_valid.npy')\n",
    "Y_test = np.load('data/Y_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see what happens if we train a FlyingSquid model without extending the labeling functions. We'll show numbers on the test set in this notebook, but in our paper we tuned on on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.8360\tPre: 0.8667\tRec: 0.7712\tF1: 0.8161\n"
     ]
    }
   ],
   "source": [
    "label_model = train_fs_model_spam(L_train_orig)\n",
    "evaluate_fs_model_spam(label_model, L_test_orig, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, precision is pretty decent, but recall lags behind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Recall with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load up embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_train = np.load('data/embeddings_train.npy')\n",
    "embeddings_valid = np.load('data/embeddings_valid.npy')\n",
    "embeddings_test = np.load('data/embeddings_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll do some preprocessing -- compute similarity scores between points, and finding the closest point to abstaining points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "\n",
    "train_to_train_sims = pairwise.cosine_similarity(embeddings_train, embeddings_train)\n",
    "valid_to_train_sims = pairwise.cosine_similarity(embeddings_valid, embeddings_train)\n",
    "test_to_train_sims = pairwise.cosine_similarity(embeddings_test, embeddings_train)\n",
    "\n",
    "mat_abstains_train, closest_pos_train, closest_neg_train = preprocess_lfs(\n",
    "    L_train_orig, L_train_orig, train_to_train_sims\n",
    ")\n",
    "mat_abstains_test, closest_pos_test, closest_neg_test = preprocess_lfs(\n",
    "    L_train_orig, L_test_orig, test_to_train_sims\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll extend the train and test matrices (note that the thresholds were tuned on the validation set -- we are eliding that step for simplicity in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these thresholds were tuned on the validation set, see paper for details\n",
    "thresholds = [0.85, 0.85, 0.85, 0.85, 0.81, 0.85, 0.85, 0.88, 0.85, 0.85]\n",
    "\n",
    "L_train_extended = extend_lfs(\n",
    "    L_train_orig,\n",
    "    mat_abstains_train, closest_pos_train, closest_neg_train,\n",
    "    thresholds\n",
    ")\n",
    "L_test_extended = extend_lfs(\n",
    "    L_test_orig,\n",
    "    mat_abstains_test, closest_pos_test, closest_neg_test,\n",
    "    thresholds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's evaluate training with the extended labeling functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.8960\tPre: 0.8898\tRec: 0.8898\tF1: 0.8898\n"
     ]
    }
   ],
   "source": [
    "label_model_extended = train_fs_model_spam(L_train_extended)\n",
    "evaluate_fs_model_spam(label_model_extended, L_test_extended, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have improved recall by more than ten points -- virtually for free!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a downstream end model\n",
    "\n",
    "And if you'd like, you can still use the label model to generate probabilistic labels for a downstream end model, which gives further improvement (see paper for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = label_model_extended.predict_proba_marginalized(L_train_extended)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flyingsquid]",
   "language": "python",
   "name": "conda-env-flyingsquid-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
